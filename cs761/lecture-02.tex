\documentclass{article}
\begin{document}
Will focus on the definition brought to light "acting rational".

\section{Agents and Environments}
Definition of an agent, types of agents, and types of environments. 

\subsection{What is an Agent?}
AI is the synthesis and analysis of \emph{computational agents} that act \emph{act intelligently}.

An \textbf{agent} is something that acts in an environment.

Agent as \textbf{actor}:
\begin{itemize}
	\item Acts autonomously in the world to achieve goals
	\item Rational - may have beliefs, desires and intentions
\end{itemize}

Agent \emph{perceives} its environment through \emph{sensors} and acts on its environment through \emph{effectors/actuators}.

\subsection{Agents as Mapping}
$f : Percept* \rightarrow Actions$, agent program runs on a physical architecture. However \textbf{the less an agent relies on this type of in-built knowledge, the more autonomous it is}.

\subsection{PAGE}
\begin{itemize}
	\item Percepts
	\item Actions
	\item Goals
	\item Environment
\end{itemize}

\subsection{Rational Agents}
Rationality depends on 
\begin{itemize}
	\item the performance measure defining the agent's degree of success
	\item the percept sequence, sequence of all the things perceived by the agent
	\item the agent's knowledge of the environment
	\item the actions that the agent can perform
\end{itemize}

We go through \textbf{PAGE} description exercises for different activities, from medical diagnosis, chess playing, movie recommendation and refinery controller.

\subsection{Rationality}
What is the \emph{right} function?

Fixed performance measure evaluates any given \textbf{sequence of environment states} examples;
\begin{itemize}
	\item one point per square cleaned up in time T?
	\item one point per clean square per time step, minus one per move?
	\item penalise for dirty squares?
\end{itemize}

Some obvious contrasts that to the lecturer drive the intuition home;
\begin{itemize}
	\item Rational $\neq$ omniscient
	\item Rational $\neq$ clairvoyant
	\item Rational $\neq$ successful
	\item Rational $\Rightarrow$ exploration, learning, autonomy
\end{itemize}

We want a unified framework that can be used to specify, characterise, compare, and contrast different AI tasks. \textbf{Tasks are the problems to which rational agents are the solutions.} To design a rational agent, we must specify the task environment via \textbf{PEAS}, performance measure, environment, actuators, sensors.

\subsection{Classifying Environment Types}

\begin{itemize}
	\item \textbf{Simulated} vs \textbf{Situated/Embodied}; the agent acting in a simulated environment vs a physical environment.
	\item \textbf{Static} vs \textbf{Dynamic}; environment doesn't change vs it changes during the agents exercise.
	\item \textbf{Discrete} vs \textbf{Continuous}; only finite percepts or actions vs infinite percepts or actions.
	\item \textbf{Fully Observable} vs \textbf{Partially Observable}; percept is complete vs only relevant information is hidden from the agent.
	\item \textbf{Deterministic} vs \textbf{Stochastic}; current state uniquely determines next state vs random elements involved.
	\item \textbf{Episodic} vs \textbf{Sequential}; every action is evaluated independently vs action evaluation being stateful.
	\item \textbf{Known} vs \textbf{Unknown}; rules of environment requires discovery vs those rules are known.
	\item \textbf{Single-Agent} vs \textbf{Multi-Agent}; 1 agent vs multiple.
\end{itemize}

\subsection{Types of Agents}

\subsubsection{Reactive}
Simple "if else", "reflexive agents". Limitations; no memory/state. May repeat same sequence of actions. Can escape from infinite loops is possible if agent introduces random actions.

\subsubsection{Model-Based}
Introduces memory into this idea of a reflexive agent. Limitations; still cannot plan into the future, it will perform poorly if a task requires any:

\begin{itemize}
	\item searching several moves ahead
	\item complex tasks requiring many individual steps
	\item logical reasoning to achieve goals
\end{itemize}

\subsubsection{Planning}
Builds on the top two, introduces a planning stage on the agents world model (provided by the model-based property). Essentially some back and fourth before applying an action. There are two types of \emph{planning agents}.

\begin{itemize}
	\item (Teleological) \textbf{Goal-based}; agents behaviour can be easily changed.
	\item \textbf{utility based agent}, fitness function that measures preferences among states of the world. Chooses action that leads to best expected utility. So in contrast to goal based, sounds like there's various "goals".
	\item Game playing agent. Opponent based, takes into consideration the optimisation an opponent will perform relative to you.
	\item Learning agent.
\end{itemize}

\end{document}
