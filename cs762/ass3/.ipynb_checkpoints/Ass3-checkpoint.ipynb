{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c291ac12",
   "metadata": {},
   "source": [
    "# Support Vector Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa09213b",
   "metadata": {},
   "source": [
    "_We copy in a utility found here, written strictly by Meng-Fen Chiang_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0927f7f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "import numpy as np\n",
    "\n",
    "def plot_svc_decision_function(model, features, labels):\n",
    "    plt.scatter(features[labels == -1, 0],\n",
    "            features[labels == -1, 1],\n",
    "            s=50, c='lightblue',\n",
    "            marker='s', edgecolor='black',\n",
    "            label='class 1')\n",
    "    plt.scatter(features[labels == 1, 0],\n",
    "            features[labels == 1, 1],\n",
    "            s=50, c='orange',\n",
    "            marker='o', edgecolor='black',\n",
    "            label='class 2')\n",
    "    \n",
    "    plt.legend(scatterpoints=1)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    ax = plt.gca()\n",
    "    xlim = ax.get_xlim()\n",
    "    ylim = ax.get_ylim()\n",
    "\n",
    "    colors = ('lightblue', 'orange', 'lightgreen', 'gray', 'cyan')\n",
    "    cmap = ListedColormap(colors[:2])\n",
    "    x1_min, x1_max = xlim[0] - 1, xlim[1] + 1\n",
    "    x2_min, x2_max = ylim[0] - 1, ylim[1] + 1\n",
    "    xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, 0.02),\n",
    "                           np.arange(x2_min, x2_max, 0.02))\n",
    "    Z = model.predict(np.array([xx1.ravel(), xx2.ravel()]).T)\n",
    "    Z = Z.reshape(xx1.shape)\n",
    "    ax.contourf(xx1, xx2, Z, alpha=0.1, cmap=cmap)    \n",
    "    \n",
    "    x = np.linspace(xlim[0], xlim[1], 30)\n",
    "    y = np.linspace(ylim[0], ylim[1], 30)\n",
    "    Y, X = np.meshgrid(y, x)\n",
    "    xy = np.vstack([X.ravel(), Y.ravel()]).T\n",
    "    P = model.decision_function(xy).reshape(X.shape)\n",
    "    \n",
    "    ax.contour(X, Y, P, colors='k',\n",
    "               levels=[-1, 0, 1], alpha=0.5,\n",
    "               linestyles=['--', '-', '--'])\n",
    "    \n",
    "    ax.set_xlim(xlim)\n",
    "    ax.set_ylim(ylim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b1615e",
   "metadata": {},
   "source": [
    "# Q1.1 - 1.4\n",
    "\n",
    "Recall $C$ references [the parameter in](https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html#sklearn-svm-linearsvc) `sklearn's` linear support vector machine implementation. We understand that $C$ drives the idea of a spectrum between a hard margin and a soft margin SVM by associating cost to misclassifcation.\n",
    "\n",
    "We carry out a leave-1-out cross-validation with an SVM, we show that SVM with  ùê∂=1 can be improved by setting  ùê∂=0.01. Reported is the train and test performance, we choose accuracy as the precise measure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4e5aacae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b255369e6710474bb7be06890073387b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='C', options=(0.1, 1000), value=0.1), Output()), _dom_classes=('wid‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from ipywidgets import interact\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "features, labels = make_blobs(n_samples=100, centers=2,\n",
    "                      random_state=0, cluster_std=1.1)\n",
    "labels = np.where(labels==0, -1, 1)\n",
    "\n",
    "def evaluatePerformance(model, features, labels):\n",
    "    train_scores, test_scores = [], []\n",
    "\n",
    "    for train_index, test_index in LeaveOneOut().split(features):\n",
    "        training_features, testing_features = features[train_index], features[test_index]\n",
    "        training_labels, testing_labels = labels[train_index], labels[test_index]\n",
    "\n",
    "        model.fit(training_features, training_labels)\n",
    "\n",
    "        training_labels_pred = model.predict(training_features)\n",
    "        train_score = accuracy_score(training_labels, training_labels_pred)\n",
    "        train_scores.append(train_score)\n",
    "\n",
    "        testing_labels_pred = model.predict(testing_features)\n",
    "        test_score = accuracy_score(testing_labels, testing_labels_pred)\n",
    "        test_scores.append(test_score)\n",
    "    \n",
    "    return np.mean(train_scores), np.mean(test_scores)\n",
    "\n",
    "def interactivePlottingOfSVM(C):\n",
    "    model = SVC(kernel='linear', C=C).fit(features, labels)\n",
    "    train_score, test_score = evaluatePerformance(model, features, labels)    \n",
    "    \n",
    "    print(\"Mean training accuracy: {:.2f}\".format(train_score))\n",
    "    print(\"Mean test accuracy: {:.2f}\".format(test_score))\n",
    "    plot_svc_decision_function(model, features, labels)\n",
    "\n",
    "interact(interactivePlottingOfSVM, C=[0.1, 1000]);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83021a30",
   "metadata": {},
   "source": [
    "# Q1.5\n",
    "\n",
    "We've designed a dataset with a 100 points for which the selection of $C$ in a linear SVM makes a difference. To dramatise the impact of $C$ we choose a dataset that has some overlap, comparing SVM's of a larger margin (low $C$) and a smaller margin (high $C$).\n",
    "\n",
    "Making performance generalisations of a differing $C$ value is dangerous, in this particular dataset though, a higher cost of misclassifcation (higher $C$) narrows the margin perhaps too much, overfitting a little too much on the training dataset (hence explaining the better training score). The lower $C$, yields a lower testing error, but looks to generalise better (higher testing score)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9f46b48",
   "metadata": {},
   "source": [
    "# Q2.1-2\n",
    "\n",
    "The investment in utilities above makes the evaluation and plotting of SVM's with differing kernel's now a problem of the past... there seems to be no motivation re: changing the evaluation method in this case given the marking criteria given.\n",
    "\n",
    "Hence we carry out the same leave-1-out cross-validation to show that `rbf` kernel is significantly more performant compared to the `linear` kernel. Hence we \"choose\" this kernel for this problem (directly answering Q2.2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bd16af4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1275f75745944e77b746b0ddcb527f69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='kernel', options=('linear', 'poly', 'rbf'), value='linear'), Outpu‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "dataset = pd.read_csv('datasets/D2.csv', header=None)\n",
    "features, labels = dataset.iloc[:, :-1].to_numpy(), dataset.iloc[:, -1].to_numpy()\n",
    "\n",
    "def interactivePlottingOfSVM(kernel):\n",
    "    model = SVC(kernel=kernel).fit(features, labels)\n",
    "    train_score, test_score = evaluatePerformance(model, features, labels)        \n",
    "    print(\"Mean training accuracy: {:.2f}\".format(train_score))\n",
    "    print(\"Mean test accuracy: {:.2f}\".format(test_score))\n",
    "    plot_svc_decision_function(model, features, labels)\n",
    "\n",
    "interact(interactivePlottingOfSVM, kernel=[\"linear\", \"poly\", \"rbf\"]);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3669c98d",
   "metadata": {},
   "source": [
    "# Q2.3\n",
    "We are clearly dealing with a non-linearly separable dataset. Points from one class are surrounded by points from the other class, forming a pattern similar to... human lungs maybe?\n",
    "\n",
    "It is well known just looking at the plot alone that the radial basis function (RBF) kernel has a higher chance of performing better. Because we know it maps the input into a higher dimensional space where the data might become linearly separable.\n",
    "\n",
    "Clearly much more viable than a straight line, or even a polynomially fitted boundary... so seeing the difference in the mean training/testing errors here shouldn't come as a surprise."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
