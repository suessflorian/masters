\documentclass{article}
\usepackage{amsmath}
\begin{document}
So far we just covered what is the most probable hypothesis given the training data. However we want to just find the {\bf most probable classifications given the training data}.

\section*{Most probable classification $\not =$ classification generated by $h_{MAP}$}
The most probable classification is obtained by combining the predictions of all hypothesis, weighted by their posterior probabilities.

\begin{align*}
	P(v_j | D) = \sum_{h_i \in H}P(v_j|h_i)P(h_i|D)
\end{align*}

Where $P(v_j|D)$ is the probability that the correct classification is $v_j$.

\section*{Bayes Optimal Classifier}
\begin{align*}
	\underset{v_j \in V}{\text{arg max }} P( v_j | D) = \underset{{v_j \in V}}{\text{arg max }} \sum_{h_i \in H} P(v_j|h_i)P(h_i|D)
\end{align*}

A theoretical classifier that effectively {\em just} combines the results of "all models", weighted by their posterior probability; the probability that the model describes the true underlying explanation of the dataset.

\section*{Naive Bayes Classifier}
Applies to learning tasks where each instance $x$ is described by a conjunction of attribute values $<a_1, a_2, ..., a_n>$ and where the target function $f(x)$ can take on any value from some finite set $V$ (the possible classification labels).

\begin{align*}
	v_{MAP} &= \underset{v_j \in V}{\text{arg max }} P(v_j | a_1, a_2, ..., a_n) \\
					&= \underset{v_j \in V}{\text{arg max }} \frac{P(a_1, a_2, ..., a_n | v_j)P(v_j)}{P(a_1, a_2, ..., a_n)} \\
					&= \underset{v_j \in V}{\text{arg max }} P(a_1, a_2, ..., a_n | v_j)P(v_j)
\end{align*}

Now $P(v_j)$ can be estimated similar to how we handle mode, the crux here is the needed simplification for $P(a_1, a_2, ..., a_n | v_j)$... The issue is finding the frequency of all instances with exactly $a_1,a_2,a_3,...,a_n$ attributes such that are classified as $v_i$... (similar approach to finding $P(v_j)$) even in the case of binary attributes, all possible instances would be $2^n$ times the number of classification labels. So as the attribute list climbs, the dataset size needed grows significantly.
 
Hence we simplify and introduce an assumption; {\em Attribute values are conditionally independent given the target value}.

Quickly reviewing types of independence with the following {\bf recursive} definitions;

\begin{itemize}
	\item Absolute independence of X,Y; $P(X,Y) = P(X|Y)P(Y) = P(X)P(Y)$
	\item Conditional independence of X,Y given Z; $P(X,Y | Z) = \\ P(X|Y, Z)P(Y | Z) = P(X|Z)P(Y|Z)$
\end{itemize}

Thus we simplify too;

\begin{align*}
P(a_1, a_2, ..., a_n | v_j) = \prod_i P(a_i | v_j)
\end{align*}

Which is important since we drastically reduce the number of terms to evaluate, and hence gives the dataset the chance to provide a good estimate. No explicit search through hypothesis space $H$, rather just counting frequencies!

So if the assumption is correct ofcourse then Naive Bayes classifications are MAP (maximal a posterior probability) classifications.

So the training phase of these classifiers is purely calculating each of these atomic probabilities right, ready for rapid multiplication later.

\section*{Smoothing}
If any of these atomic probabilities evaluates to zero, we have a very serious destruction of information. Since $x\cdot0=0$ ofcourse... thus there are techniques, such as {\bf Laplace smoothing} that effectively modifies each atomic probability in the following way; 

\begin{align*}
P(A_k = a_i | v_j) = \frac{n_{ij} + 1}{nj + m} 
\end{align*}

\begin{itemize}
	\item $n_{ij}$ is the number of training examples with class labels $v_j$ and attribute label $a_j$
	\item $n_j$ is the number of training examples with class label $v_j$
	\item $m$ is the number of unique attributes values of $A_k$
\end{itemize}

There's also the issue of missing attribute values for new instances... all we do there is just omit those in the calculations. Makes complete sense as we are not gaining information from the fact that these variables are missing.

Extends beyond just a {\em few missing variables}... if all are missing, this approach reduces to the mode classifier.

\end{document}
