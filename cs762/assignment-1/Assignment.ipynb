{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d3bf6de0",
   "metadata": {},
   "source": [
    "# (Task 1) Importing and filling the data\n",
    "We're going to be abusing `pandas`, a pretty [standard tool for managing datasets for analysis](https://pandas.pydata.org/docs/). For the importing of the data we'll just read csv files into [dataframes](https://pandas.pydata.org/docs/user_guide/dsintro.html#basics-dataframe) directly and handle missing values [leaning on `pandas`](https://pandas.pydata.org/docs/user_guide/missing_data.html) by specifying the \"NA values\", and filling them in via respective column means. \n",
    "\n",
    "This assignment gives us the impression that data is presented nicely to us. But in reality... that data cleaning is a very sizeable task in of itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e04f24b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn import tree\n",
    "\n",
    "\n",
    "# WARN: critical assumption here is missing data is denoted via '?', this was provided via Piazza clarification at\n",
    "# at the time of the assignment, but we would absolutely need to do some data cleaning.\n",
    "wb_df = pd.read_csv('datasets/website-phishing.csv', na_values='?') \n",
    "bcp_df = pd.read_csv('datasets/BCP.csv', na_values='?') \n",
    "ary_df = pd.read_csv('datasets/arrhythmia.csv', na_values='?') \n",
    "\n",
    "wb_df = wb_df.fillna(wb_df.mean())\n",
    "bcp_df = bcp_df.fillna(bcp_df.mean())\n",
    "ary_df = ary_df.fillna(ary_df.median()) # TODO: comment why\n",
    "\n",
    "imports = [\n",
    "    {\"name\":\"website-phishing\", \"df\":wb_df},\n",
    "    {\"name\":\"bcp\", \"df\":bcp_df},\n",
    "    {\"name\":\"arrythmia\", \"df\":ary_df},\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e8e0d7e",
   "metadata": {},
   "source": [
    "# Training/Testing Sets\n",
    "We will split here the training and testing datasets. Going old school with the 70/30 split. Not much to commentate here other than pointing out that it's important to;\n",
    "\n",
    "- Shuffle the dataset, to remove any potential sorting that exists (mitigating training bias) and,\n",
    "- Adhering to the golden rule of not polluting the training efforts with our testing dataset.\n",
    "\n",
    "Ofcourse `scikit-learn` has a utility (aptly named [train_test_split](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html)) here. As it very well should be, it's literally one of the most common procedures in these excercises to perform).\n",
    "\n",
    "We finish here by conveniently seperating the features and labels too for easy access too (`tr_` prefixed to represent training)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fecc6cb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Overall size of the website-phishing full dataset: 342705\n",
      "Size of training set: 239878 (%69.99547715965626%)\n",
      "Size of testing set: 102827 (%30.004522840343732%)\n",
      "\n",
      "\n",
      "-- Overall size of the bcp full dataset: 7513\n",
      "Size of training set: 5258 (%69.98535871156662%)\n",
      "Size of testing set: 2255 (%30.01464128843338%)\n",
      "\n",
      "\n",
      "-- Overall size of the arrythmia full dataset: 126560\n",
      "Size of training set: 88480 (%69.91150442477876%)\n",
      "Size of testing set: 38080 (%30.08849557522124%)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class LabelsFeatures:\n",
    "    features: pd.DataFrame\n",
    "    labels: pd.DataFrame\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Dataset:\n",
    "    name: str\n",
    "    dataframe_: pd.DataFrame\n",
    "    testing: LabelsFeatures\n",
    "    training: LabelsFeatures\n",
    "\n",
    "full_datasets = []\n",
    "\n",
    "for dataset in imports:\n",
    "    name, df = dataset[\"name\"], dataset[\"df\"]\n",
    "    print(f\"-- Overall size of the {name} full dataset: {df.size}\")\n",
    "\n",
    "    [training, testing] = train_test_split(df, shuffle=True, train_size=0.7, test_size=0.3)\n",
    "    print(f\"Size of training set: {training.size} (%{training.size/df.size * 100}%)\")\n",
    "    print(f\"Size of testing set: {testing.size} (%{testing.size/df.size * 100}%)\")\n",
    "    print(\"\\n\")\n",
    "\n",
    "    tr_features, tr_labels = training.iloc[:, :-1], training.iloc[:, -1]\n",
    "    te_features, te_labels = testing.iloc[:, :-1], testing.iloc[:, -1]\n",
    "    full_datasets.append(Dataset(\n",
    "        name=name,\n",
    "        dataframe_=df,\n",
    "        training=LabelsFeatures(features=tr_features, labels=tr_labels),\n",
    "        testing=LabelsFeatures(features=te_features, labels=te_labels)\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cdea98b",
   "metadata": {},
   "source": [
    "# (Task 2a & 2b) Decision Stump & Unpruned Tree\n",
    "And so we begin! I seperated these two subtasks out because they are adequately bland. But for future reference; for decision tree refresher I'd read the `scikit-learn` [chapter on decision trees](https://scikit-learn.org/stable/modules/tree.html). As we're about to dive right into the realms of our friend the [`DecisionTreeClassifier`](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html).\n",
    "\n",
    "To pay respects to the lecturer JÃ¶rg Simon Wicker hosted in Summer-Autumn of 2023, we will fix the node splitting strategy for the length of this notebook to `criterion=\"entropy\"`. That is quantifying data \"impurity\" via a flavour of the very well explained \"entropy\" function (Shannon entropy is used in `sci-kit learn`).\n",
    "\n",
    "The [generalised definition](https://scikit-learn.org/stable/modules/tree.html#classification-criteria) is used here as we have datasets that have more than two possible labels (important distinction to make since we looked at only the binary classification definitions of entropy in the course).\n",
    "\n",
    "In conjunction to fixing the `criterion` we will also for the length of this document fix `random_state=0` and `splitter=\"best\"` as the implementation of the `DecisionTreeClassifier` within `scikit-learn` introduces randomness within selection of candidate features by default otherwise. We want deterministic behaviour to align with the teachings of this course. Hence all `DecisionTreeClassifier` instances will be set via;\n",
    "```\n",
    "DecisionTreeClassifier(random_state=0, criterion=\"entropy\", splitter=\"best\" ...)\n",
    "```\n",
    "\n",
    "The decision stump (called `stump_classifier` below) is just a decision tree classifier model with depth one. Now we're _technically_ assembling this tree via a \"pre-pruning\" technique of setting a `max_depth` of the tree, although sufficient for demonstrational purposes. The completely unpruned decision tree (called `unpruned_classifier` below) is tree that has no `max_depth` set (and no other hyperparameters set that would otherwise restrict the growth of this tree). Hence we have the following;\n",
    "\n",
    "_NOTE: we log out the depth of the underlying constructed tree of the classifiers (perhaps unconventionally) via the property `.tree_.max_depth`_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "90abd321",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- For dataset website-phishing\n",
      "The depth of tree assembled for stump_classifier: 1\n",
      "The depth of the tree assembled for the unpruned_classifier: 24\n",
      "\n",
      "\n",
      "-- For dataset bcp\n",
      "The depth of tree assembled for stump_classifier: 1\n",
      "The depth of the tree assembled for the unpruned_classifier: 6\n",
      "\n",
      "\n",
      "-- For dataset arrythmia\n",
      "The depth of tree assembled for stump_classifier: 1\n",
      "The depth of the tree assembled for the unpruned_classifier: 11\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def getTrainedStumpClassifier(ds: Dataset):\n",
    "    stump_classifier = tree.DecisionTreeClassifier(\n",
    "        random_state=0,\n",
    "        criterion=\"entropy\",\n",
    "        splitter=\"best\",\n",
    "        max_depth=1)\n",
    "    stump_classifier.fit(ds.training.features, ds.training.labels)\n",
    "    return stump_classifier\n",
    "\n",
    "def getTrainedUnprunedClassifier(ds: Dataset):\n",
    "    unpruned_classifier = tree.DecisionTreeClassifier(\n",
    "        random_state=0,\n",
    "        criterion=\"entropy\",\n",
    "        splitter=\"best\")\n",
    "    unpruned_classifier.fit(ds.training.features, ds.training.labels)\n",
    "    return unpruned_classifier\n",
    "\n",
    "for ds in full_datasets:\n",
    "    print(\"-- For dataset\", ds.name)\n",
    "\n",
    "    stump_classifier = getTrainedStumpClassifier(ds)\n",
    "    print(\"The depth of tree assembled for stump_classifier:\", stump_classifier.tree_.max_depth)\n",
    "\n",
    "    unpruned_classifier = getTrainedUnprunedClassifier(ds)\n",
    "    print(\"The depth of the tree assembled for the unpruned_classifier:\", unpruned_classifier.tree_.max_depth)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5cb3807",
   "metadata": {},
   "source": [
    "# (Task 2c) A Post Pruned Tree\n",
    "We enter \"pruning\" the general technique of trimming the decision tree down in order to tackle the problem of \"overfitting\" - concretely in this section we'll take this `unpruned_tree` and apply the \"cost complexity pruning\" technique onto it.\n",
    "\n",
    "Resources about this post pruning technique (as it's a novel technique relative to what has been covered in lectures);\n",
    "- [Short video by StatQuest contextualised via Regression Tree's](https://www.youtube.com/watch?v=D0efHEJsfHo&t)\n",
    "- [Chapter from `scikit-learn` docs directly](https://scikit-learn.org/stable/auto_examples/tree/plot_cost_complexity_pruning.html)\n",
    "\n",
    "It is a parameterised technique (takes in a \"cost complexity\" parameter, $\\alpha$), hence open as a tunable hyper parameter. But the idea is that the general magnitude of $\\alpha$ impacts how **aggressively** we prune this tree (_how many nodes we trim back_).\n",
    "\n",
    "Technically, a post pruned tree here for sake of demonstration can simply be a product of this tree and a \"strong enough\" alpha to prune at least one decision node. Arbitrarily choosing this value is slightly innacurate as we need to garuantee some sort of actual \"pruning\" right? So we utulise this handy method on the instance of a `DecisionTreeClassifer` called `cost_complexity_pruning_path` that lists out possible alpha variables that _make an impact_. And from this selection we can arbitrarily choose one (for sake of demonstration) and apply the prune."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "45f2425c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- For dataset website-phishing\n",
      "Total number of trimmed nodes:  364\n",
      "\n",
      "\n",
      "-- For dataset bcp\n",
      "Total number of trimmed nodes:  10\n",
      "\n",
      "\n",
      "-- For dataset arrythmia\n",
      "Total number of trimmed nodes:  56\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "def getCandidateCCPParameters(ds: Dataset):\n",
    "    classifier = tree.DecisionTreeClassifier(random_state=0, criterion=\"entropy\", splitter=\"best\")\n",
    "    path = classifier.cost_complexity_pruning_path(ds.training.features, ds.training.labels)\n",
    "    return path.ccp_alphas    \n",
    "\n",
    "for ds in full_datasets:\n",
    "    print(\"-- For dataset\", ds.name)\n",
    "\n",
    "    # NOTE: this purely a demonstration, we'd not just randomly choose the cost complexity pruning alpha like this\n",
    "    chosen_ccp_alpha = random.choice(getCandidateCCPParameters(ds)) \n",
    "\n",
    "    post_pruned_classifier = tree.DecisionTreeClassifier(\n",
    "        random_state=0,\n",
    "        criterion=\"entropy\",\n",
    "        splitter=\"best\",\n",
    "        ccp_alpha=chosen_ccp_alpha\n",
    "    )\n",
    "    post_pruned_classifier.fit(ds.training.features, ds.training.labels)\n",
    "    \n",
    "    print(\"Total number of trimmed nodes: \", getTrainedUnprunedClassifier(ds).tree_.node_count - post_pruned_classifier.tree_.node_count)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c33b84",
   "metadata": {},
   "source": [
    "# (Task 3) Hyperparameter (in our case just $\\alpha$ ) search\n",
    "Now the real cool shit (and the data science really begins). We want expand on what we found in our post pruned tree above... that is finding the $\\alpha$ that _performs the best_ because above we surfaced a series of candidates (provided by the above `cost_complexity_pruning_path`), but just picked a random one!\n",
    "\n",
    "So how can we find the best $\\alpha$? Well we can split the training data set (that is still **strictly separated** from our `testing` dataset way at the beginning of this notebook) into the 70:30 ratio again, building two new sets, call them the `training` and `validation` sets. We'd do this because that allows us to iterate through each candidate $\\alpha$ here, train it on the `training` set and test the result of this $\\alpha$ tuned model on the `validation` set.\n",
    "\n",
    "But as pointed out in the lecture notes, this restricts our training dataset quite a lot and impacts the ability for our models to generalise well (as our training dataset becomes less andÂ less representative of our overall global dataset the smaller it gets). This caveat here is really only visible when contrasted against the [cross-validation](https://en.wikipedia.org/wiki/Cross-validation_(statistics)) technique.\n",
    "\n",
    "We covered this technique thoroughly in course lectures, effectively, we end up evaluating the regulated learning process under supervision of an $\\alpha$ while training the model on the entire `training` set without this second layer of splitting using _folding_. In `scikit-learn`, we have the [`GridSearchCV`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html) utility; where given a list of hyper parameters (and their respective candidate values), it performs an exhaustive candidate hyperparameter combination search, returning to us the best hyper parameter combination for the `testing` set provided. It does this by iterating through all combinations of hyper parameters and evaluating those hyper parameters by k-fold cross validation. Each run giving an aggregated (mean) accuracy score of the model trained under the regulation of those hyperparameters, later used for comparing different hyperparameter combination effectiveness.\n",
    "\n",
    "We in particular have to define the strategy used to evaluate the performance of each $\\alpha$ tuned model... the list of strategies available in `GridSearchCV` are [listed here](https://scikit-learn.org/stable/modules/model_evaluation.html#common-cases-predefined-values). Since we are dealing with basic classification we opt for the `accuracy` scoring method, that is, given your $i$th predicted value $\\hat y_i$ with the associated true label $y_i$, and keeping in mind the sample count $n_{samples}$;\n",
    "\n",
    "$$accuracy(y,\\hat y) = \\frac{\\sum_{i=0}^{n_{samples}-1}1(\\hat y_i = y_i)}{n_{samples}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27237cf2",
   "metadata": {},
   "source": [
    "TODO: We can go a step further and perform what is known as a nested cross-validation.\n",
    "\n",
    "- [Chapter in `scikit-learn` that covers an exemplar nested cross-validation run](https://scikit-learn.org/stable/auto_examples/model_selection/plot_nested_cross_validation_iris.html)\n",
    "- [Video that helps describe the intuition behind nested cross validation](https://www.youtube.com/watch?v=az60jS7MQhU)\n",
    "\n",
    "\n",
    "## Once we find this best $\\alpha$\n",
    "\n",
    "We build a final model, we call this one `best_performing_post_pruned_classifier` and use it as our strongest candidate that is using the \"cost complexity pruning\" technique. \n",
    "\n",
    "Some resources for further reading;\n",
    "\n",
    "- [Chapter from `scitkit-learn` about hyper parameter tuning](https://scikit-learn.org/stable/modules/grid_search.html)\n",
    "- [More practical to the point medium article on hyper parameter tuning](https://medium.com/chinmaygaikwad/hyperparameter-tuning-for-tree-models-f99a66446742)\n",
    "\n",
    "_Note: in general, the more folds (`cv` param in `GridSearchCV`), the more representative the model accuracy would be of the real accuracy. The tradeoff here is patience, more accuracy requires more `cv` which in turn requires more time... I arbitrarilly chose 10 fold._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e955cef8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- For dataset website-phishing\n",
      "Cost complexity pruning alpha candidates:  258\n",
      "Best \"cost complexity alpha\": 2.6726039741597194e-05\n",
      "Total number of trimmed nodes:  10\n",
      "\n",
      "\n",
      "-- For dataset bcp\n",
      "Cost complexity pruning alpha candidates:  12\n",
      "Best \"cost complexity alpha\": 0.0\n",
      "Total number of trimmed nodes:  0\n",
      "\n",
      "\n",
      "-- For dataset arrythmia\n",
      "Cost complexity pruning alpha candidates:  56\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/floriansuess/.local/share/virtualenvs/assignment-1-Qb4KhU98/lib/python3.11/site-packages/sklearn/model_selection/_split.py:700: UserWarning: The least populated class in y has only 2 members, which is less than n_splits=10.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best \"cost complexity alpha\": 0.05366311167175622\n",
      "Total number of trimmed nodes:  98\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from typing import List\n",
    "\n",
    "def getFittedAlphaTunedClassifier(ds: Dataset, alphaCandidates: List[float]):\n",
    "    base_classifier = tree.DecisionTreeClassifier(random_state=0, criterion=\"entropy\", splitter=\"best\")\n",
    "    params = {'ccp_alpha': alphaCandidates}\n",
    "    search = GridSearchCV(\n",
    "        estimator=base_classifier,\n",
    "        param_grid=params,\n",
    "        scoring='accuracy',\n",
    "        cv=10,\n",
    "        n_jobs=-1,\n",
    "    )\n",
    "    search.fit(ds.training.features, ds.training.labels)\n",
    "    \n",
    "    best_performing_post_pruned_classifier = tree.DecisionTreeClassifier(\n",
    "        random_state=0,\n",
    "        criterion=\"entropy\",\n",
    "        splitter=\"best\",\n",
    "        ccp_alpha=search.best_params_['ccp_alpha']\n",
    "    )\n",
    "    \n",
    "    best_performing_post_pruned_classifier.fit(ds.training.features, ds.training.labels)\n",
    "    return best_performing_post_pruned_classifier\n",
    "\n",
    "for ds in full_datasets:\n",
    "    print(\"-- For dataset\", ds.name)\n",
    "\n",
    "    candidates = getCandidateCCPParameters(ds)\n",
    "    print(\"Cost complexity pruning alpha candidates: \", len(candidates))\n",
    "   \n",
    "    best_performing_post_pruned_classifier = getFittedAlphaTunedClassifier(ds, candidates)\n",
    "   \n",
    "    print('Best \"cost complexity alpha\":',  best_performing_post_pruned_classifier.get_params()['ccp_alpha'])\n",
    "\n",
    "    print(\"Total number of trimmed nodes: \", getTrainedUnprunedClassifier(ds).tree_.node_count - best_performing_post_pruned_classifier.tree_.node_count)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b05d1b",
   "metadata": {},
   "source": [
    "# (Task 4) Evaluating these Models\n",
    "So now that we have iterated on the `post_pruned_classifier` and ended up with `best_performing_post_pruned_classifier`. It's time to formally evaluate these three models... the one mentioned and the basic `stump_classifier` and `unpruned_classifier` models. The absolute performance of these models are to be considered and you can quite easily fit each model on the `training` data provided and go ahead and predict the labels on these unseen `testing` features, finding the [`accuracy_score`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html) of each model and comparing this way (then there is the matter of evaluating precision, and recall etc... etc...)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5182bb87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best performing post pruned classifier accuracy: %95.77931866144105\n",
      "Unpruned classifier classifier accuracy: %95.77931866144105\n",
      "Stump classifier classifier accuracy: %89.4181489297558\n",
      "\n",
      "\n",
      "Best performing post pruned classifier accuracy: %94.6341463414634\n",
      "Unpruned classifier classifier accuracy: %94.6341463414634\n",
      "Stump classifier classifier accuracy: %92.19512195121952\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/floriansuess/.local/share/virtualenvs/assignment-1-Qb4KhU98/lib/python3.11/site-packages/sklearn/model_selection/_split.py:700: UserWarning: The least populated class in y has only 2 members, which is less than n_splits=10.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best performing post pruned classifier accuracy: %58.82352941176471\n",
      "Unpruned classifier classifier accuracy: %57.35294117647059\n",
      "Stump classifier classifier accuracy: %50.0\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "for ds in full_datasets:\n",
    "    print(\"-- For dataset\", ds.name)\n",
    "    \n",
    "    predicted_labels = getFittedAlphaTunedClassifier(ds, getCandidateCCPParameters(ds)).predict(ds.testing.features)\n",
    "    score = accuracy_score(ds.testing.labels, predicted_labels)\n",
    "    print(f\"Best performing post pruned classifier accuracy: %{score*100}\")\n",
    "\n",
    "    predicted_labels = getTrainedUnprunedClassifier(ds).predict(ds.testing.features)\n",
    "    score = accuracy_score(ds.testing.labels, predicted_labels)\n",
    "    print(f\"Unpruned classifier classifier accuracy: %{score*100}\")\n",
    "\n",
    "    predicted_labels = getTrainedStumpClassifier(ds).predict(ds.testing.features)\n",
    "    score = accuracy_score(ds.testing.labels, predicted_labels)\n",
    "    print(f\"Stump classifier classifier accuracy: %{score*100}\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cdc5ab2",
   "metadata": {},
   "source": [
    "## Evaluating the statistical significance of pairwise model performance differences \n",
    "\n",
    "We should evaluate the difference between each model... as by chance a model could be performing well relative to another. How do we rule this chance out? We look at two models we're comparing, say $M_1$ and $M_2$ and set the null hypothesis to; $M_1$ are the same $M_2$, and using a [t-test](https://en.wikipedia.org/wiki/Student%27s_t-test), if we can reject the null hypothesis (ie, p-value < 0.05), then we can conclude the difference between $M_1$ and $M_2$ is **statistically significant**.\n",
    "\n",
    "- [Quick video on the inards of the t-test for independent samples for those unfamiliar](https://www.youtube.com/watch?v=c9ombGmaEy8)\n",
    "\n",
    "We need a series of like of like scoring methods, in our case we can keep using the standard `accuracy` scoring. We can generate these scores via something like a 10-fold cross validation, where we add the `accuracy` score of each iteration onto the series of scores $S_{M_1}$ and $S_{M_2}$ and utilise [`ttest_ind`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.ttest_ind.html#scipy-stats-ttest-ind) found within `scipy` in order calculate the T-test for the means of two independent samples of scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0405f07c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "from scipy import stats\n",
    "\n",
    "scores = cross_validate(best_performing_post_pruned_classifier, tr_features, tr_labels, scoring='accuracy', cv=10)\n",
    "tuned_scores = scores['test_score']\n",
    "\n",
    "scores = cross_validate(stump_classifier, tr_features, tr_labels, scoring='accuracy', cv=10)\n",
    "stump_scores = scores['test_score']\n",
    "\n",
    "t_stat, p_value = stats.ttest_ind(tuned_scores, stump_scores)\n",
    "print(\"P-Value: \", p_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "027fbbf1",
   "metadata": {},
   "source": [
    "# (Task 5) Pre-Pruned Tree\n",
    "In contrast to our simple section in \"A Post Pruned Tree\", we will look into exclusively some pre-pruning techniques that actively restrict the growth of the tree as it builds. The hyperparameters we will look at will arbitrarilly be `max_depth` and to highlight further how `GridSearchCV` works, also consider in conjunction a series of `min_samples_leaf` and also `min_samples_split`.\n",
    "\n",
    "Candidates for `max_depth` will be just `[1, 2, ..., unpruned.tree_.max_depth]` and for `min_samples_leaf`, `min_samples_split` to be `[1, 5, 10, 20, 50, 100, 200, 500]`.\n",
    "\n",
    "_Note: as you can see, we can definitely search within an abritrarily sized hyperparameter space_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "378f4b07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- For dataset website-phishing\n",
      "Fitting 10 folds for each of 88 candidates, totalling 880 fits\n",
      "Best \"max_depth\": 11\n",
      "Best \"min_samples_leaf\": 1\n",
      "Best performing pre pruned classifier accuracy: %94.54326198372023\n",
      "\n",
      "\n",
      "-- For dataset bcp\n",
      "Fitting 10 folds for each of 88 candidates, totalling 880 fits\n",
      "Best \"max_depth\": 6\n",
      "Best \"min_samples_leaf\": 1\n",
      "Best performing pre pruned classifier accuracy: %94.6341463414634\n",
      "\n",
      "\n",
      "-- For dataset arrythmia\n",
      "Fitting 10 folds for each of 88 candidates, totalling 880 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/floriansuess/.local/share/virtualenvs/assignment-1-Qb4KhU98/lib/python3.11/site-packages/sklearn/model_selection/_split.py:700: UserWarning: The least populated class in y has only 2 members, which is less than n_splits=10.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best \"max_depth\": 4\n",
      "Best \"min_samples_leaf\": 10\n",
      "Best performing pre pruned classifier accuracy: %61.029411764705884\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def getFittedTunedPrePrunedClassifier(ds: Dataset):\n",
    "    base_classifier = tree.DecisionTreeClassifier(random_state=0, criterion=\"entropy\", splitter=\"best\")\n",
    "    params = {\n",
    "        'max_depth': [i for i in range(1, unpruned_classifier.tree_.max_depth+1)],\n",
    "        'min_samples_leaf': [1, 5, 10, 20, 50, 100, 200, 500],\n",
    "    }\n",
    "    search = GridSearchCV(\n",
    "        estimator=base_classifier,\n",
    "        n_jobs=-1,\n",
    "        param_grid=params,\n",
    "        cv=10,\n",
    "        verbose=True,\n",
    "        scoring='accuracy'\n",
    "    )\n",
    "    search.fit(ds.training.features, ds.training.labels)\n",
    "\n",
    "    best_performing_pre_pruned_classifier = tree.DecisionTreeClassifier(\n",
    "        random_state=0,\n",
    "        criterion=\"entropy\",\n",
    "        splitter=\"best\",\n",
    "        max_depth=search.best_params_['max_depth'],\n",
    "        min_samples_leaf=search.best_params_['min_samples_leaf'],\n",
    "    )\n",
    "    \n",
    "    best_performing_pre_pruned_classifier.fit(ds.training.features, ds.training.labels)\n",
    "    return best_performing_pre_pruned_classifier\n",
    "\n",
    "for ds in full_datasets:\n",
    "    print(\"-- For dataset\", ds.name)\n",
    "    \n",
    "    best_performing_pre_pruned_classifier = getFittedTunedPrePrunedClassifier(ds)\n",
    "    print('Best \"max_depth\":', best_performing_pre_pruned_classifier.get_params()['max_depth'])\n",
    "    print('Best \"min_samples_leaf\":', best_performing_pre_pruned_classifier.get_params()['min_samples_leaf'])\n",
    "\n",
    "    predicted_labels = best_performing_pre_pruned_classifier.predict(ds.testing.features)\n",
    "    score = accuracy_score(ds.testing.labels, predicted_labels)\n",
    "    print(f\"Best performing pre pruned classifier accuracy: %{score*100}\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a801bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
