{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d3bf6de0",
   "metadata": {},
   "source": [
    "# (Task 1) Importing and filling the data\n",
    "We're going to be abusing `pandas`, a pretty [standard tool for managing datasets for analysis](https://pandas.pydata.org/docs/). For the importing of the data we'll just read csv files into [dataframes](https://pandas.pydata.org/docs/user_guide/dsintro.html#basics-dataframe) directly and handle missing values [leaning on `pandas`](https://pandas.pydata.org/docs/user_guide/missing_data.html) by specifying the \"NA values\", and filling them in via respective column means. \n",
    "\n",
    "This assignment gives us the impression that data is presented nicely to us. But in reality I hear from colleagues, is that data cleaning is a very sizeable task in of itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e04f24b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn import tree\n",
    "\n",
    "# WARN: critical assumption here is missing data is denoted via '?', this was provided via Piazza clarification at\n",
    "# at the time of the assignment, but we would absolutely need to do some data cleaning.\n",
    "df = pd.read_csv('datasets/website-phishing.csv', na_values='?') \n",
    "df = df.fillna(df.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e8e0d7e",
   "metadata": {},
   "source": [
    "# Training/Testing Sets\n",
    "We will split here the training and testing datasets. Going old school with the 70/30 split. Not much to commentate here other than pointing out that it's important to;\n",
    "\n",
    "- Shuffle the dataset, to remove any potential sorting that exists (mitigating training bias) and,\n",
    "- Adhering to the golden rule of not polluting the training efforts with our testing dataset.\n",
    "\n",
    "Ofcourse `scikit-learn` has a utility (aptly named [train_test_split](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html)) here. As it very well should be, it's literally one of the most common procedures in these excercises to perform).\n",
    "\n",
    "We finish here by conveniently seperating the features and labels too for easy access too (`tr_` prefixed to represent training)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fecc6cb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "overall size of the full dataset:  342705\n",
      "size of training set: 239878 (%69.99547715965626%)\n",
      "size of testing set: 102827 (%30.004522840343732%)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "print(\"overall size of the full dataset: \", df.size)\n",
    "[training, testing] = train_test_split(df, shuffle=True, train_size=0.7, test_size=0.3)\n",
    "print(f\"size of training set: {training.size} (%{training.size/df.size * 100}%)\")\n",
    "print(f\"size of testing set: {testing.size} (%{testing.size/df.size * 100}%)\")\n",
    "\n",
    "tr_features, tr_labels = testing.iloc[:, :-1], testing.iloc[:, -1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cdea98b",
   "metadata": {},
   "source": [
    "# (Task 2a & 2b) Decision Stump & Unpruned Tree\n",
    "And so we begin! I seperated these two subtasks out because they are adequately bland. But for future reference; for decision tree refresher I'd read the `scikit-learn` [chapter on decision trees](https://scikit-learn.org/stable/modules/tree.html). As we're about to dive right into the realms of our friend the [`DecisionTreeClassifier`](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html).\n",
    "\n",
    "To pay respects to the lecturer JÃ¶rg Simon Wicker hosted in Summer-Autumn of 2023, we will fix the node splitting strategy for the length of this notebook to `criterion=\"entropy\"`. That is quantifying data \"impurity\" via a flavour of the very well explained \"entropy\" function (Shannon entropy is used in `sci-kit learn`).\n",
    "\n",
    "The [generalised definition](https://scikit-learn.org/stable/modules/tree.html#classification-criteria) is used here as we have datasets that have more than two possible labels (important distinction to make since we looked at only the binary classification definitions of entropy in the course).\n",
    "\n",
    "In conjunction to fixing the `criterion` we will also for the length of this document fix `random_state=0` and `splitter=\"best\"` as the implementation of the `DecisionTreeClassifier` within `scikit-learn` introduces randomness within selection of candidate features by default otherwise. We want deterministic behaviour to align with the teachings of this course. Hence all `DecisionTreeClassifier` instances will be set via;\n",
    "```\n",
    "DecisionTreeClassifier(random_state=0, criterion=\"entropy\", splitter=\"best\" ...)\n",
    "```\n",
    "\n",
    "The decision stump (called `stump_classifier` below) is just a decision tree classifier model with depth one. Now we're _technically_ assembling this tree via a \"pre-pruning\" technique of setting a `max_depth` of the tree, although sufficient for demonstrational purposes. The completely unpruned decision tree (called `unpruned_classifier` below) is tree that has no `max_depth` set (and no other hyperparameters set that would restrict the growth of this tree). Hence we have the following;\n",
    "\n",
    "_NOTE: we log out the depth of the underlying constructed tree of the classifiers (perhaps unconventionally) via the property `.tree_.max_depth`_\n",
    "\n",
    "_NOTE: the implementation of the `DecisionTreeClassifier` within `scikit-learn` introduces randomness via the technique of splitting, hence we fix this from now on too via setting the `random_state` parameter._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "90abd321",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the depth of tree assembled for stump_classifier: 1\n",
      "the depth of the tree assembled for the unpruned_classifier: 22\n"
     ]
    }
   ],
   "source": [
    "stump_classifier = tree.DecisionTreeClassifier(random_state=0, criterion=\"entropy\", splitter=\"best\", max_depth=1)\n",
    "stump_classifier.fit(tr_features, tr_labels)\n",
    "print(\"the depth of tree assembled for stump_classifier:\", stump_classifier.tree_.max_depth)\n",
    "\n",
    "unpruned_classifier = tree.DecisionTreeClassifier(random_state=0, criterion=\"entropy\", splitter=\"best\")\n",
    "unpruned_classifier.fit(tr_features, tr_labels)\n",
    "print(\"the depth of the tree assembled for the unpruned_classifier:\", unpruned_classifier.tree_.max_depth)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5cb3807",
   "metadata": {},
   "source": [
    "# (Task 2c) A Post Pruned Tree\n",
    "We opt for a post-pruned tree, that is we'll take this `unpruned_tree` and apply the \"cost complexity pruning\" technique onto it.\n",
    "\n",
    "Resources about this post pruning technique (as it's a novel technique relative to what has been covered in lectures);\n",
    "- [Short video by StatQuest contextualised via Regression Tree's](https://www.youtube.com/watch?v=D0efHEJsfHo&t)\n",
    "- [Chapter from `scikit-learn` docs directly](https://scikit-learn.org/stable/auto_examples/tree/plot_cost_complexity_pruning.html)\n",
    "\n",
    "It is a parameterised technique (takes in a \"cost complexity\" parameter, $\\alpha$), hence open as a possible hyper parameter. But the idea is that the general magnitude of $\\alpha$ impacts how **aggressively** we prune this tree (_how many nodes we trim back_).\n",
    "\n",
    "_NOTE: to drive the intuition of how this alpha hyper parameter works, there exists a large enough alpha such that the final output tree would be a decision stump yet again._\n",
    "\n",
    "Technically, a post pruned tree here for sake of demonstration can simply be a product of this tree and a \"strong enough\" alpha to prune at least one decision node. Arbitrarily choosing this value is slightly innacurate as we need to garuantee some sort of actual \"pruning\" right? So we utulise this handy method on the instance of a `DecisionTreeClassifer` called `cost_complexity_pruning_path` that lists out possible alpha variables that _make an impact_. And from this selection we can arbitrarily choose one and apply the prune."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "45f2425c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'unpruned_classifier' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m post_pruned_classifier \u001b[38;5;241m=\u001b[39m tree\u001b[38;5;241m.\u001b[39mDecisionTreeClassifier(random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, criterion\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mentropy\u001b[39m\u001b[38;5;124m\"\u001b[39m, splitter\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbest\u001b[39m\u001b[38;5;124m\"\u001b[39m, ccp_alpha\u001b[38;5;241m=\u001b[39mchosen_ccp_alpha)\n\u001b[1;32m     10\u001b[0m post_pruned_classifier\u001b[38;5;241m.\u001b[39mfit(tr_features, tr_labels)\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtotal number of trimmed nodes: \u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[43munpruned_classifier\u001b[49m\u001b[38;5;241m.\u001b[39mtree_\u001b[38;5;241m.\u001b[39mnode_count \u001b[38;5;241m-\u001b[39m post_pruned_classifier\u001b[38;5;241m.\u001b[39mtree_\u001b[38;5;241m.\u001b[39mnode_count)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'unpruned_classifier' is not defined"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "classifier = tree.DecisionTreeClassifier(random_state=0, criterion=\"entropy\", splitter=\"best\")\n",
    "path = classifier.cost_complexity_pruning_path(tr_features, tr_labels)\n",
    "\n",
    "# NOTE: this purely a demonstration, we'd not just randomly choose the cost complexity pruning alpha like this\n",
    "chosen_ccp_alpha = random.choice(path.ccp_alphas) \n",
    "\n",
    "post_pruned_classifier = tree.DecisionTreeClassifier(random_state=0, criterion=\"entropy\", splitter=\"best\", ccp_alpha=chosen_ccp_alpha)\n",
    "post_pruned_classifier.fit(tr_features, tr_labels)\n",
    "print(\"total number of trimmed nodes: \", unpruned_classifier.tree_.node_count - post_pruned_classifier.tree_.node_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c33b84",
   "metadata": {},
   "source": [
    "# (Task 3) Hyper parameter search\n",
    "Now the real cool shit (and the data science really begins). We want expand on what we found in our post pruned tree above... that is finding the $\\alpha$ that _performs the best_ because above we surfaced a series of candidates (provided by the above `cost_complexity_pruning_path`), but just picked a random one!\n",
    "\n",
    "So how can we find the best $\\alpha$? Well we can split the training data set (that is still **strictly separated** from our `testing` dataset way at the beginning of this notebook) into the 70:30 ratio again, building two new sets, call them the `training` and `validation` sets. We'd do this because that allows us to iterate through each candidate $\\alpha$ here, train it on the `training` set and test the result of this $\\alpha$ tuned model on the `validation` set.\n",
    "\n",
    "But as pointed out in the lecture notes, this restricts our training dataset quite a lot and impacts the ability for our models to generalise well (as our training dataset becomes less andÂ less representative of our overall global dataset the smaller it gets). This caveat here is really only visible when contrasted against the [cross-validation](https://en.wikipedia.org/wiki/Cross-validation_(statistics)) technique ðŸ™Œ.\n",
    "\n",
    "We covered this technique thoroughly in course lectures, effectively, we end up evaluating the regulated learning process under supervision of an $\\alpha$ whilst correctly testing it. Wasting nothing of our original `testing` data set (at the cost of some addded compute resource).\n",
    "\n",
    "In `scikit-learn` we utulise what is known as the [`GridSearchCV`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html) utility which does what we want here for us. Effectively taking in a list of \"grids\" to check exhaustively candidate parameters, and neatly summarising after completion the outcome, surfacing the best parameter to use.\n",
    "\n",
    "Some resources for further reading;\n",
    "\n",
    "- [Chapter from `scitkit-learn` about hyper parameter tuning](https://scikit-learn.org/stable/modules/grid_search.html)\n",
    "- [More practical to the point medium article on hyper parameter tuning](https://medium.com/chinmaygaikwad/hyperparameter-tuning-for-tree-models-f99a66446742)\n",
    "\n",
    "_Note: in general, the more folds (`cv` param in `GridSearchCV`), the more representative the model accuracy would be of the real error. The tradeoff here is patience, more accuracy requires more time... but I'm lucky here, I have a really powerful machine_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e955cef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "\n",
    "print(\"cost complexity pruning alpha candidates: \", len(path.ccp_alphas))\n",
    "params = {'ccp_alpha': path.ccp_alphas}\n",
    "\n",
    "base_classifier = tree.DecisionTreeClassifier(random_state=0, criterion=\"entropy\", splitter=\"best\")\n",
    "GS = GridSearchCV(estimator=base_classifier, n_jobs=-1 param_grid = params,cv=LeaveOneOut(),verbose=True, scoring='accuracy')\n",
    "GS.fit(tr_features, tr_labels)\n",
    "\n",
    "print('Best Parameters:',GS.best_params_,end='\\n\\n')\n",
    "print('Best Score:',GS.best_score_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
