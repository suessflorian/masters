{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d3bf6de0",
   "metadata": {},
   "source": [
    "# (Task 1) Importing and filling the data\n",
    "We're going to be abusing `pandas`, a pretty [standard tool for managing datasets for analysis](https://pandas.pydata.org/docs/). For the importing of the data we'll just read csv files into [dataframes](https://pandas.pydata.org/docs/user_guide/dsintro.html#basics-dataframe) directly and handle missing values [leaning on `pandas`](https://pandas.pydata.org/docs/user_guide/missing_data.html) by specifying the \"NA values\", and filling them in via respective column means. \n",
    "\n",
    "This assignment gives us the impression that data is presented nicely to us. But in reality... that data cleaning is a very sizeable task in of itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5e04f24b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn import tree\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# WARN: critical assumption here is missing data is denoted via '?', this was provided via Piazza clarification at\n",
    "# at the time of the assignment, but we would absolutely need to do some data cleaning.\n",
    "wb_df = pd.read_csv('datasets/website-phishing.csv', na_values='?') \n",
    "bcp_df = pd.read_csv('datasets/BCP.csv', na_values='?') \n",
    "ary_df = pd.read_csv('datasets/arrhythmia.csv', na_values='?') \n",
    "\n",
    "wb_df = wb_df.fillna(wb_df.mean())\n",
    "bcp_df = bcp_df.fillna(bcp_df.mean())\n",
    "ary_df = ary_df.fillna(ary_df.mean())\n",
    "\n",
    "imports = [\n",
    "    {\"name\":\"website-phishing\", \"df\":wb_df},\n",
    "    {\"name\":\"bcp\", \"df\":bcp_df},\n",
    "    {\"name\":\"arrythmia\", \"df\":ary_df},\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e8e0d7e",
   "metadata": {},
   "source": [
    "# Training/Testing Sets\n",
    "We will split here the training and testing datasets. Going old school with the 70/30 split. Not much to commentate here other than pointing out that it's important to;\n",
    "\n",
    "- Shuffle the dataset, to remove any potential sorting that exists (mitigating training bias) and,\n",
    "- Adhering to the golden rule of not polluting the training efforts with our testing dataset.\n",
    "\n",
    "Ofcourse `scikit-learn` has a utility (aptly named [train_test_split](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html)) here. As it very well should be, it's literally one of the most common procedures in these excercises to perform).\n",
    "\n",
    "We introduce a couple lightweight [dataclasses](https://docs.python.org/3/library/dataclasses.html) to help structure the handling and iterating of task requirements on each `Dataset`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fecc6cb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Overall row count of the website-phishing full dataset: 11055\n",
      "-- With overall column count: 31\n",
      "Size of training set: 7738 (%69.99547715965626%)\n",
      "Size of testing set: 3317 (%30.004522840343732%)\n",
      "\n",
      "\n",
      "-- Overall row count of the bcp full dataset: 683\n",
      "-- With overall column count: 11\n",
      "Size of training set: 478 (%69.98535871156662%)\n",
      "Size of testing set: 205 (%30.01464128843338%)\n",
      "\n",
      "\n",
      "-- Overall row count of the arrythmia full dataset: 452\n",
      "-- With overall column count: 280\n",
      "Size of training set: 316 (%69.91150442477876%)\n",
      "Size of testing set: 136 (%30.08849557522124%)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class LabelsFeatures:\n",
    "    features: pd.DataFrame\n",
    "    labels: pd.DataFrame\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Dataset:\n",
    "    name: str\n",
    "    dataframe_: pd.DataFrame\n",
    "    testing: LabelsFeatures\n",
    "    training: LabelsFeatures\n",
    "\n",
    "full_datasets = []\n",
    "\n",
    "for dataset in imports:\n",
    "    name, df = dataset[\"name\"], dataset[\"df\"]\n",
    "    rows, columns = df.shape\n",
    "    print(f\"-- Overall row count of the {name} full dataset: {rows}\")\n",
    "    print(f\"-- With overall column count: {columns}\")\n",
    "\n",
    "    [training, testing] = train_test_split(df, shuffle=True, train_size=0.7, test_size=0.3)\n",
    "    print(f\"Size of training set: {training.shape[0]} (%{training.shape[0]/rows * 100}%)\")\n",
    "    print(f\"Size of testing set: {testing.shape[0]} (%{testing.shape[0]/rows * 100}%)\")\n",
    "    print(\"\\n\")\n",
    "\n",
    "    tr_features, tr_labels = training.iloc[:, :-1], training.iloc[:, -1]\n",
    "    te_features, te_labels = testing.iloc[:, :-1], testing.iloc[:, -1]\n",
    "    full_datasets.append(Dataset(\n",
    "        name=name,\n",
    "        dataframe_=df,\n",
    "        training=LabelsFeatures(features=tr_features, labels=tr_labels),\n",
    "        testing=LabelsFeatures(features=te_features, labels=te_labels)\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cdea98b",
   "metadata": {},
   "source": [
    "# (Task 2a & 2b) Decision Stump & Unpruned Tree\n",
    "And so we begin! I seperated these two subtasks out because they are adequately bland. But for future reference; for decision tree refresher I'd read the `scikit-learn` [chapter on decision trees](https://scikit-learn.org/stable/modules/tree.html). As we're about to dive right into the realms of our friend the [`DecisionTreeClassifier`](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html).\n",
    "\n",
    "To pay respects to the lecturer Jörg Simon Wicker hosted in Summer-Autumn of 2023, we will fix the node splitting strategy for the length of this notebook to `criterion=\"entropy\"`. That is quantifying data \"impurity\" via a flavour of the very well explained \"entropy\" function (Shannon entropy is used in `sci-kit learn`).\n",
    "\n",
    "The [generalised definition](https://scikit-learn.org/stable/modules/tree.html#classification-criteria) is used here as we have datasets that have more than two possible labels (important distinction to make since we looked at only the binary classification definitions of entropy in the course).\n",
    "\n",
    "In conjunction to fixing the `criterion` we will also for the length of this document fix `random_state=0` and `splitter=\"best\"` as the implementation of the `DecisionTreeClassifier` within `scikit-learn` introduces randomness within selection of candidate features by default otherwise. We want deterministic behaviour to align with the teachings of this course. Hence all `DecisionTreeClassifier` instances will be set via;\n",
    "```\n",
    "DecisionTreeClassifier(random_state=0, criterion=\"entropy\", splitter=\"best\" ...)\n",
    "```\n",
    "\n",
    "The decision stump (called `stump_classifier` below) is just a decision tree classifier model with depth one. Now we're _technically_ assembling this tree via a \"pre-pruning\" technique of setting a `max_depth` of the tree, although sufficient for demonstrational purposes. The completely unpruned decision tree (called `unpruned_classifier` below) is tree that has no `max_depth` set (and no other hyperparameters set that would otherwise restrict the growth of this tree). Hence we have the following;\n",
    "\n",
    "_NOTE: we log out the depth of the underlying constructed tree of the classifiers (perhaps unconventionally) via the property `.tree_.max_depth`_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "90abd321",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- For dataset website-phishing\n",
      "The depth of tree assembled for stump_classifier: 1\n",
      "The depth of the tree assembled for the unpruned_classifier: 22\n",
      "\n",
      "\n",
      "-- For dataset bcp\n",
      "The depth of tree assembled for stump_classifier: 1\n",
      "The depth of the tree assembled for the unpruned_classifier: 9\n",
      "\n",
      "\n",
      "-- For dataset arrythmia\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input X contains NaN.\nDecisionTreeClassifier does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 21\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ds \u001b[38;5;129;01min\u001b[39;00m full_datasets:\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-- For dataset\u001b[39m\u001b[38;5;124m\"\u001b[39m, ds\u001b[38;5;241m.\u001b[39mname)\n\u001b[0;32m---> 21\u001b[0m     stump_classifier \u001b[38;5;241m=\u001b[39m \u001b[43mgetTrainedStumpClassifier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe depth of tree assembled for stump_classifier:\u001b[39m\u001b[38;5;124m\"\u001b[39m, stump_classifier\u001b[38;5;241m.\u001b[39mtree_\u001b[38;5;241m.\u001b[39mmax_depth)\n\u001b[1;32m     24\u001b[0m     unpruned_classifier \u001b[38;5;241m=\u001b[39m getTrainedUnprunedClassifier(ds)\n",
      "Cell \u001b[0;32mIn[3], line 7\u001b[0m, in \u001b[0;36mgetTrainedStumpClassifier\u001b[0;34m(ds)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgetTrainedStumpClassifier\u001b[39m(ds: Dataset):\n\u001b[1;32m      2\u001b[0m     stump_classifier \u001b[38;5;241m=\u001b[39m tree\u001b[38;5;241m.\u001b[39mDecisionTreeClassifier(\n\u001b[1;32m      3\u001b[0m         random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m      4\u001b[0m         criterion\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mentropy\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      5\u001b[0m         splitter\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbest\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      6\u001b[0m         max_depth\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m----> 7\u001b[0m     \u001b[43mstump_classifier\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m stump_classifier\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/assignment-1-Qb4KhU98/lib/python3.11/site-packages/sklearn/tree/_classes.py:889\u001b[0m, in \u001b[0;36mDecisionTreeClassifier.fit\u001b[0;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[1;32m    859\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, check_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m    860\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Build a decision tree classifier from the training set (X, y).\u001b[39;00m\n\u001b[1;32m    861\u001b[0m \n\u001b[1;32m    862\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    886\u001b[0m \u001b[38;5;124;03m        Fitted estimator.\u001b[39;00m\n\u001b[1;32m    887\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 889\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    890\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    891\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    892\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    893\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheck_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    894\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    895\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/assignment-1-Qb4KhU98/lib/python3.11/site-packages/sklearn/tree/_classes.py:186\u001b[0m, in \u001b[0;36mBaseDecisionTree.fit\u001b[0;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[1;32m    184\u001b[0m check_X_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(dtype\u001b[38;5;241m=\u001b[39mDTYPE, accept_sparse\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsc\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    185\u001b[0m check_y_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(ensure_2d\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m--> 186\u001b[0m X, y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    187\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidate_separately\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcheck_X_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck_y_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    188\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m issparse(X):\n\u001b[1;32m    190\u001b[0m     X\u001b[38;5;241m.\u001b[39msort_indices()\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/assignment-1-Qb4KhU98/lib/python3.11/site-packages/sklearn/base.py:579\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    577\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mestimator\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m check_X_params:\n\u001b[1;32m    578\u001b[0m     check_X_params \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdefault_check_params, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_X_params}\n\u001b[0;32m--> 579\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mX\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcheck_X_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    580\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mestimator\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m check_y_params:\n\u001b[1;32m    581\u001b[0m     check_y_params \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdefault_check_params, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_y_params}\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/assignment-1-Qb4KhU98/lib/python3.11/site-packages/sklearn/utils/validation.py:921\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m    915\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    916\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound array with dim \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m expected <= 2.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    917\u001b[0m             \u001b[38;5;241m%\u001b[39m (array\u001b[38;5;241m.\u001b[39mndim, estimator_name)\n\u001b[1;32m    918\u001b[0m         )\n\u001b[1;32m    920\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m force_all_finite:\n\u001b[0;32m--> 921\u001b[0m         \u001b[43m_assert_all_finite\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    922\u001b[0m \u001b[43m            \u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    923\u001b[0m \u001b[43m            \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    924\u001b[0m \u001b[43m            \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    925\u001b[0m \u001b[43m            \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_all_finite\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mallow-nan\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    926\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    928\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ensure_min_samples \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    929\u001b[0m     n_samples \u001b[38;5;241m=\u001b[39m _num_samples(array)\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/assignment-1-Qb4KhU98/lib/python3.11/site-packages/sklearn/utils/validation.py:161\u001b[0m, in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m estimator_name \u001b[38;5;129;01mand\u001b[39;00m input_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m has_nan_error:\n\u001b[1;32m    145\u001b[0m     \u001b[38;5;66;03m# Improve the error message on how to handle missing values in\u001b[39;00m\n\u001b[1;32m    146\u001b[0m     \u001b[38;5;66;03m# scikit-learn.\u001b[39;00m\n\u001b[1;32m    147\u001b[0m     msg_err \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    148\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not accept missing values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    149\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m encoded as NaN natively. For supervised learning, you might want\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    159\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#estimators-that-handle-nan-values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    160\u001b[0m     )\n\u001b[0;32m--> 161\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg_err)\n",
      "\u001b[0;31mValueError\u001b[0m: Input X contains NaN.\nDecisionTreeClassifier does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values"
     ]
    }
   ],
   "source": [
    "def getTrainedStumpClassifier(ds: Dataset):\n",
    "    stump_classifier = tree.DecisionTreeClassifier(\n",
    "        random_state=0,\n",
    "        criterion=\"entropy\",\n",
    "        splitter=\"best\",\n",
    "        max_depth=1)\n",
    "    stump_classifier.fit(ds.training.features, ds.training.labels)\n",
    "    return stump_classifier\n",
    "\n",
    "def getTrainedUnprunedClassifier(ds: Dataset):\n",
    "    unpruned_classifier = tree.DecisionTreeClassifier(\n",
    "        random_state=0,\n",
    "        criterion=\"entropy\",\n",
    "        splitter=\"best\")\n",
    "    unpruned_classifier.fit(ds.training.features, ds.training.labels)\n",
    "    return unpruned_classifier\n",
    "\n",
    "for ds in full_datasets:\n",
    "    print(\"-- For dataset\", ds.name)\n",
    "\n",
    "    stump_classifier = getTrainedStumpClassifier(ds)\n",
    "    print(\"The depth of tree assembled for stump_classifier:\", stump_classifier.tree_.max_depth)\n",
    "\n",
    "    unpruned_classifier = getTrainedUnprunedClassifier(ds)\n",
    "    print(\"The depth of the tree assembled for the unpruned_classifier:\", unpruned_classifier.tree_.max_depth)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5cb3807",
   "metadata": {},
   "source": [
    "# (Task 2c) A Post Pruned Tree\n",
    "We enter \"pruning\" the general technique of trimming the decision tree down in order to tackle the problem of \"overfitting\" - concretely in this section we'll take this `unpruned_tree` and apply the \"cost complexity pruning\" technique onto it.\n",
    "\n",
    "Resources about this post pruning technique (as it's a novel technique relative to what has been covered in lectures);\n",
    "- [Short video by StatQuest contextualised via Regression Tree's](https://www.youtube.com/watch?v=D0efHEJsfHo&t)\n",
    "- [Chapter from `scikit-learn` docs directly](https://scikit-learn.org/stable/auto_examples/tree/plot_cost_complexity_pruning.html)\n",
    "\n",
    "It is a parameterised technique (takes in a \"cost complexity\" parameter, $\\alpha$), hence open as a tunable hyper parameter. But the idea is that the general magnitude of $\\alpha$ impacts how **aggressively** we prune this tree (_how many nodes we trim back_).\n",
    "\n",
    "Technically, a post pruned tree here for sake of demonstration can simply be a product of this tree and a \"strong enough\" alpha to prune at least one decision node. Arbitrarily choosing this value is slightly innacurate as we need to garuantee some sort of actual \"pruning\" right? So we utulise this handy method on the instance of a `DecisionTreeClassifer` called `cost_complexity_pruning_path` that lists out possible alpha variables that _make an impact_. And from this selection we can arbitrarily choose one (for sake of demonstration) and apply the prune.\n",
    "\n",
    "_NOTE: we conveniently wrap the candidate selection via `getCandidateCCPParameters`, I have removed the first candidate of the ccp alphas path as this value has no impact, and I want to force \"some\" sort of post pruning for the sake of clear demonstration and comparision._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f2425c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def getCandidateCCPParameters(ds: Dataset):\n",
    "    classifier = tree.DecisionTreeClassifier(random_state=0, criterion=\"entropy\", splitter=\"best\")\n",
    "    path = classifier.cost_complexity_pruning_path(ds.training.features, ds.training.labels)\n",
    "    return path.ccp_alphas[1:]\n",
    "\n",
    "for ds in full_datasets:\n",
    "    print(\"-- For dataset\", ds.name)\n",
    "\n",
    "    # NOTE: this purely a demonstration, we'd not just randomly choose the cost complexity pruning alpha like this\n",
    "    chosen_ccp_alpha = random.choice(getCandidateCCPParameters(ds)) \n",
    "\n",
    "    post_pruned_classifier = tree.DecisionTreeClassifier(\n",
    "        random_state=0,\n",
    "        criterion=\"entropy\",\n",
    "        splitter=\"best\",\n",
    "        ccp_alpha=chosen_ccp_alpha\n",
    "    )\n",
    "    post_pruned_classifier.fit(ds.training.features, ds.training.labels)\n",
    "    \n",
    "    print(\"Total number of trimmed nodes: \", getTrainedUnprunedClassifier(ds).tree_.node_count - post_pruned_classifier.tree_.node_count)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c33b84",
   "metadata": {},
   "source": [
    "# (Task 3) Hyperparameter (in our case just $\\alpha$ ) search\n",
    "Now the real cool shit (and the data science really begins). We want expand on what we found in our post pruned tree above... that is finding the $\\alpha$ that _performs the best_ because above we surfaced a series of candidates (provided by the above `cost_complexity_pruning_path`), but just picked a random one!\n",
    "\n",
    "So how can we find the best $\\alpha$? Well we can split the training data set (that is still **strictly separated** from our `testing` dataset way at the beginning of this notebook) into the 70:30 ratio again, building two new sets, call them the `training` and `validation` sets. We'd do this because that allows us to iterate through each candidate $\\alpha$ here, train it on the `training` set and test the result of this $\\alpha$ tuned model on the `validation` set.\n",
    "\n",
    "But as pointed out in the lecture notes, this restricts our training dataset quite a lot and impacts the ability for our models to generalise well (as our training dataset becomes less and less representative of our overall global dataset the smaller it gets). This caveat here is really only visible when contrasted against the [cross-validation](https://en.wikipedia.org/wiki/Cross-validation_(statistics)) technique.\n",
    "\n",
    "We covered this technique thoroughly in course lectures, effectively, we end up evaluating the regulated learning process under supervision of an $\\alpha$ while training the model on the entire `training` set without this second layer of splitting using _folding_. In `scikit-learn`, we have the [`GridSearchCV`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html) utility; where given a list of hyper parameters (and their respective candidate values), it performs an exhaustive candidate hyperparameter combination search, returning to us the best hyper parameter combination for the `testing` set provided. It does this by iterating through all combinations of hyper parameters and evaluating those hyper parameters by k-fold cross validation. Each run giving an aggregated (mean) accuracy score of the model trained under the regulation of those hyperparameters, later used for comparing different hyperparameter combination effectiveness.\n",
    "\n",
    "We in particular have to define the strategy used to evaluate the performance of each $\\alpha$ tuned model... the list of strategies available in `GridSearchCV` are [listed here](https://scikit-learn.org/stable/modules/model_evaluation.html#common-cases-predefined-values). Since we are dealing with basic classification we opt for the `accuracy` scoring method, that is, given your $i$th predicted value $\\hat y_i$ with the associated true label $y_i$, and keeping in mind the sample count $n_{samples}$;\n",
    "\n",
    "$$accuracy(y,\\hat y) = \\frac{\\sum_{i=0}^{n_{samples}-1}1(\\hat y_i = y_i)}{n_{samples}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27237cf2",
   "metadata": {},
   "source": [
    "TODO: We can go a step further and perform what is known as a nested cross-validation.\n",
    "\n",
    "- [Chapter in `scikit-learn` that covers an exemplar nested cross-validation run](https://scikit-learn.org/stable/auto_examples/model_selection/plot_nested_cross_validation_iris.html)\n",
    "- [Video that helps describe the intuition behind nested cross validation](https://www.youtube.com/watch?v=az60jS7MQhU)\n",
    "\n",
    "\n",
    "## Once we find this best $\\alpha$\n",
    "\n",
    "We build a final model, we call this one `best_performing_post_pruned_classifier` and use it as our strongest candidate that is using the \"cost complexity pruning\" technique. \n",
    "\n",
    "Some resources for further reading;\n",
    "\n",
    "- [Chapter from `scitkit-learn` about hyper parameter tuning](https://scikit-learn.org/stable/modules/grid_search.html)\n",
    "- [More practical to the point medium article on hyper parameter tuning](https://medium.com/chinmaygaikwad/hyperparameter-tuning-for-tree-models-f99a66446742)\n",
    "\n",
    "_Note: in general, the more folds (`cv` param in `GridSearchCV`), the more representative the model accuracy would be of the real accuracy. The tradeoff here is patience, more accuracy requires more `cv` which in turn requires more time... I arbitrarilly chose 10 fold._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e955cef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from typing import List\n",
    "\n",
    "def getFittedAlphaTunedClassifier(ds: Dataset, alphaCandidates: List[float]):\n",
    "    base_classifier = tree.DecisionTreeClassifier(random_state=0, criterion=\"entropy\", splitter=\"best\")\n",
    "    params = {'ccp_alpha': alphaCandidates}\n",
    "    search = GridSearchCV(\n",
    "        estimator=base_classifier,\n",
    "        param_grid=params,\n",
    "        scoring='accuracy',\n",
    "        cv=10,\n",
    "        n_jobs=-1,\n",
    "    )\n",
    "    search.fit(ds.training.features, ds.training.labels)\n",
    "    \n",
    "    best_performing_post_pruned_classifier = tree.DecisionTreeClassifier(\n",
    "        random_state=0,\n",
    "        criterion=\"entropy\",\n",
    "        splitter=\"best\",\n",
    "        ccp_alpha=search.best_params_['ccp_alpha']\n",
    "    )\n",
    "    \n",
    "    best_performing_post_pruned_classifier.fit(ds.training.features, ds.training.labels)\n",
    "    return best_performing_post_pruned_classifier\n",
    "\n",
    "for ds in full_datasets:\n",
    "    print(\"-- For dataset\", ds.name)\n",
    "\n",
    "    candidates = getCandidateCCPParameters(ds)\n",
    "    print(\"Cost complexity pruning alpha candidates: \", len(candidates))\n",
    "   \n",
    "    best_performing_post_pruned_classifier = getFittedAlphaTunedClassifier(ds, candidates)\n",
    "   \n",
    "    print('Best \"cost complexity alpha\":             ',  best_performing_post_pruned_classifier.get_params()['ccp_alpha'])\n",
    "\n",
    "    print(\"Total number of trimmed nodes:            \", getTrainedUnprunedClassifier(ds).tree_.node_count - best_performing_post_pruned_classifier.tree_.node_count)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b05d1b",
   "metadata": {},
   "source": [
    "# (Task 4) Evaluating these Models\n",
    "So now that we have iterated on the `post_pruned_classifier` and ended up with `best_performing_post_pruned_classifier`. It's time to formally evaluate these three models... the one mentioned and the basic `stump_classifier` and `unpruned_classifier` models. The absolute performance of these models are to be considered and you can quite easily fit each model on the `training` data provided and go ahead and predict the labels on these unseen `testing` features, finding the [`accuracy_score`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html) of each model and comparing this way (then there is the matter of evaluating precision, and recall etc... etc...)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5182bb87",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "for ds in full_datasets:\n",
    "    print(\"-- For dataset\", ds.name)\n",
    "    \n",
    "    predicted_labels = getFittedAlphaTunedClassifier(ds, getCandidateCCPParameters(ds)).predict(ds.testing.features)\n",
    "    score = accuracy_score(ds.testing.labels, predicted_labels)\n",
    "    print(f\"Best performing post pruned classifier accuracy: %{score*100}\")\n",
    "\n",
    "    predicted_labels = getTrainedUnprunedClassifier(ds).predict(ds.testing.features)\n",
    "    score = accuracy_score(ds.testing.labels, predicted_labels)\n",
    "    print(f\"Unpruned classifier classifier accuracy:         %{score*100}\")\n",
    "\n",
    "    predicted_labels = getTrainedStumpClassifier(ds).predict(ds.testing.features)\n",
    "    score = accuracy_score(ds.testing.labels, predicted_labels)\n",
    "    print(f\"Stump classifier classifier accuracy:            %{score*100}\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cdc5ab2",
   "metadata": {},
   "source": [
    "## Evaluating the statistical significance of pairwise model performance differences \n",
    "\n",
    "We should evaluate the difference between each model... as by chance a model could be performing well relative to another. How do we rule this chance out? We look at two models we're comparing, say $M_1$ and $M_2$ and set the null hypothesis to; $M_1$ are the same $M_2$, and using a [t-test](https://en.wikipedia.org/wiki/Student%27s_t-test), if we can reject the null hypothesis (ie, p-value < 0.05), then we can conclude the difference between $M_1$ and $M_2$ is **statistically significant**.\n",
    "\n",
    "- [Quick video on the inards of the t-test for independent samples for those unfamiliar](https://www.youtube.com/watch?v=c9ombGmaEy8)\n",
    "\n",
    "We need a series of like of like scoring methods, in our case we can keep using the standard `accuracy` scoring. We can generate these scores via something like a 10-fold cross validation, where we add the `accuracy` score of each iteration onto the series of scores $S_{M_1}$ and $S_{M_2}$ and utilise [`ttest_ind`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.ttest_ind.html#scipy-stats-ttest-ind) found within `scipy` in order calculate the T-test for the means of two independent samples of scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0405f07c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "from scipy import stats\n",
    "\n",
    "for ds in full_datasets:\n",
    "    print(\"-- For dataset\", ds.name)\n",
    "    \n",
    "    scores = cross_validate(\n",
    "        getFittedAlphaTunedClassifier(ds, getCandidateCCPParameters(ds)),\n",
    "        ds.training.features,\n",
    "        ds.training.labels,\n",
    "        scoring='accuracy',\n",
    "        cv=10\n",
    "    )\n",
    "    tuned_post_pruned_scores = scores['test_score']\n",
    "\n",
    "    scores = cross_validate(\n",
    "        getTrainedStumpClassifier(ds),\n",
    "        ds.training.features,\n",
    "        ds.training.labels,\n",
    "        scoring='accuracy',\n",
    "        cv=10\n",
    "    )\n",
    "    stump_scores = scores['test_score']\n",
    "    \n",
    "    scores = cross_validate(\n",
    "        getTrainedUnprunedClassifier(ds),\n",
    "        ds.training.features,\n",
    "        ds.training.labels,\n",
    "        scoring='accuracy',\n",
    "        cv=10\n",
    "    )\n",
    "    unpruned_scores = scores['test_score']\n",
    "\n",
    "\n",
    "    _, p_value = stats.ttest_ind(tuned_post_pruned_scores, unpruned_scores)\n",
    "    print(\"P-Value for independent t-test (tuned post pruned classifier, unpruned classifier): \", p_value)\n",
    "    _, p_value = stats.ttest_ind(tuned_post_pruned_scores, stump_scores)\n",
    "    print(\"P-Value for independent t-test (tuned post pruned classifier, stump classifier)   : \", p_value)\n",
    "    _, p_value = stats.ttest_ind(unpruned_scores, stump_scores)\n",
    "    print(\"P-Value for independent t-test          (unpruned classifier, stump classifier)   : \", p_value)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "027fbbf1",
   "metadata": {},
   "source": [
    "# (Task 5) Pre-Pruned Tree\n",
    "In contrast to our simple section in \"A Post Pruned Tree\", we will look into exclusively some pre-pruning techniques that actively restrict the growth of the tree as it builds. The hyperparameters we will look at will arbitrarilly be `max_depth` and to highlight further how `GridSearchCV` works, also consider in conjunction a series of `min_samples_leaf` and also `min_samples_split`.\n",
    "\n",
    "Candidates for `max_depth` will be just `[1, 2, ..., unpruned.tree_.max_depth]` and for `min_samples_leaf`, `min_samples_split` to be `[1, 5, 10, 20, 50, 100, 200, 500]`.\n",
    "\n",
    "_Note: as you can see, we can definitely search within an abritrarily sized hyperparameter space_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "378f4b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFittedTunedPrePrunedClassifier(ds: Dataset):\n",
    "    base_classifier = tree.DecisionTreeClassifier(random_state=0, criterion=\"entropy\", splitter=\"best\")\n",
    "    params = {\n",
    "        'max_depth': [i for i in range(1, unpruned_classifier.tree_.max_depth+1)],\n",
    "        'min_samples_leaf': [1, 5, 10, 20, 50, 100, 200, 500],\n",
    "    }\n",
    "    search = GridSearchCV(\n",
    "        estimator=base_classifier,\n",
    "        n_jobs=-1,\n",
    "        param_grid=params,\n",
    "        cv=10,\n",
    "        verbose=True,\n",
    "        scoring='accuracy'\n",
    "    )\n",
    "    search.fit(ds.training.features, ds.training.labels)\n",
    "\n",
    "    best_performing_pre_pruned_classifier = tree.DecisionTreeClassifier(\n",
    "        random_state=0,\n",
    "        criterion=\"entropy\",\n",
    "        splitter=\"best\",\n",
    "        max_depth=search.best_params_['max_depth'],\n",
    "        min_samples_leaf=search.best_params_['min_samples_leaf'],\n",
    "    )\n",
    "    \n",
    "    best_performing_pre_pruned_classifier.fit(ds.training.features, ds.training.labels)\n",
    "    return best_performing_pre_pruned_classifier\n",
    "\n",
    "for ds in full_datasets:\n",
    "    print(\"-- For dataset\", ds.name)\n",
    "    \n",
    "    preClassifier = getFittedTunedPrePrunedClassifier(ds)\n",
    "    print('Best \"max_depth\":', preClassifier.get_params()['max_depth'])\n",
    "    print('Best \"min_samples_leaf\":', preClassifier.get_params()['min_samples_leaf'])\n",
    "    \n",
    "    predicted_labels = preClassifier.predict(ds.testing.features)\n",
    "    score = accuracy_score(ds.testing.labels, predicted_labels)\n",
    "    print(f\"Best performing pre pruned classifier accuracy: %{score*100}\")\n",
    "\n",
    "    postClassifier = getFittedAlphaTunedClassifier(ds, getCandidateCCPParameters(ds))\n",
    "    predicted_labels = postClassifier.predict(ds.testing.features)\n",
    "    score = accuracy_score(ds.testing.labels, predicted_labels)\n",
    "    print(f\"Best performing post pruned classifier accuracy: %{score*100}\")\n",
    "          \n",
    "    scores = cross_validate(\n",
    "        preClassifier,\n",
    "        ds.training.features,\n",
    "        ds.training.labels,\n",
    "        scoring='accuracy',\n",
    "        cv=10\n",
    "    )\n",
    "    preScores = scores['test_score']\n",
    "    \n",
    "    scores = cross_validate(\n",
    "        postClassifier,\n",
    "        ds.training.features,\n",
    "        ds.training.labels,\n",
    "        scoring='accuracy',\n",
    "        cv=10\n",
    "    )      \n",
    "    postScores = scores['test_score']\n",
    "    _, p_value = stats.ttest_ind(preScores, postScores)\n",
    "    print(\"P-Value for independent t-test (pre pruned strategy, post pruned strategy): \", p_value)\n",
    "    \n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a801bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
